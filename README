1. Lychee13-3634 Dataset
Since the limitation of file size, we put our Lychee13-3634 Dataset on Baidu NetDisk Link: https://pan.baidu.com/s/1AZ8XMhzjkbrPDHTyuFaZ-g?pwd=1rxd
Extracting code: 1rxd
-Lychee13-3634(13 varieties and 3634 images)
  -train
    -Blakleaf        (204)
    -ChickenMouth    (208)
    -Crystalball     (204)
    -Feizixiao       (234)
    -Guiwei          (187)
    -HangingGreen    (297)
    -LycheeKing      (209)
    -Nuomici         (217)
    -Seedless        (231)
    -TopScorerRed    (253)
    -WhiteSugerPoppy (220)
    -Whitewax        (207)
    -Xianjingfeng    (222)
  -test 
    -Blakleaf        (52)
    -ChickenMouth    (52)
    -Crystalball     (52)
    -Feizixiao       (59)
    -Guiwei          (47)
    -HangingGreen    (75)
    -LycheeKing      (53)
    -Nuomici         (55)
    -Seedless        (58)
    -TopScorerRed    (64)
    -WhiteSugerPoppy (55)
    -Whitewax        (52)
    -Xianjingfeng    (56)
NOTE: ./MultipleLycheeImages.png and ./SingleLycheeImages.png show the 13 categories of multiple and single lychee image samples from Lychee13-3634 dataset, respectively.

2. We comprehensively analysis 20 exsiting SOTA methods on lychee classification task, including 6 ResNet-series, 9 Deep learning-series, and 5 Yolo-series. 
1) ResNet-series are including:
  -ResNet152
  -ResNet101
  -ResNet50
  -ResNet34
  -ResNet18
  -Residual-Attention-Network
2) Deep learning-series are including:
  -EfficientNetV2-s
  -EfficientNetV2-m
  -EfficientNetV2-l
  -SENet
  -VisionTransformer
  -SqueezeNet
  -ShuffleNetV2
  -MobileNetV2
NOTE: All the resource codes of above models are available at: https://github.com/jyanhuang/Lychee13-3634.git

3) Yolo-series codes are including:
  -YOLOv5
  -YOLOv7
  -YOLOv8
  -YOLOv9
  -YOLOv10
NOTE: because the uploading limitation of GitHub, we put the codes of YOLO-series in Baidu NetDist Link: 
Extracting code: 

References
[1] 
[2] He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition, 2016b, pp. 770â€“778.
[3] 
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15] 

